{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d487a634-f541-465c-8e1b-c3bd7aa99fce",
   "metadata": {},
   "source": [
    "# Autonomous Literature Review (AutoLit) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6eea6fc-5500-4b12-b15b-5ed6f96b1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test data\n",
    "title = \"Feasibility of Artificial Intelligence Driven Analysis in the Context of Nepalese Legal System\"\n",
    "abstract = \"We proposed an innovative solution through an Artificial Intelligence driven legal analysis customized to the utility of the Nepalese legal context. Using advanced machine learning (ML) models and Retrieval-Augmented Generation (RAG) techniques, the research provides legal insights, streamlines judicial processes, and enhances accessibility to legal information. The legal documents were processed to convert into JSON format, and then to convert into vector data. GPT-4o was used for query expansion and response generation, whereas text embeddings were generated through text-embedding-ada-002. Key features include efficient document retrieval and query expansion for enhanced search precision. The model performs well across different query types, achieving an ð¹1 score of 0.797 for rule-recall, 0.857 for rhetorical understanding, and 0.875 for interpretation-based queries. This work marks a significant step towards integrating AI into the legal domain of Nepal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6b270b9-9ca1-4465-9f2c-81f2dd0d6b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5586560a-dfbe-4709-bb1d-dde563b0ebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI legal analysis', 'Nepalese legal system', 'Machine learning', 'RAG', 'Document retrieval', 'Query expansion', 'GPT-4o', 'Text embeddings']\n"
     ]
    }
   ],
   "source": [
    "# Keyword Generation\n",
    "import os\n",
    "from fastapi import HTTPException\n",
    "import requests\n",
    "from typing import Dict, List\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def keyword_gen(title: str, abstract: str) -> list[str]:\n",
    "    GEMINI_API_URL = os.environ.get(\"GEMINI_API_URL\") or \"\"\n",
    "    res = requests.post(GEMINI_API_URL, json={\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\n",
    "                \"text\": f\"\"\"\n",
    "                    Extract 5â€“10 relevant and concise keywords from the following\n",
    "                    research paper title and abstract. Each keyword should be 1â€“3\n",
    "                    words long. Return only the keywords as a plain list, one per\n",
    "                    line. Don't say anything else.\n",
    "\n",
    "                    Title: {title}\n",
    "\n",
    "                    Abstract: {abstract}\n",
    "                \"\"\"\n",
    "                }]\n",
    "            }]\n",
    "    })\n",
    "\n",
    "    if not res.ok:\n",
    "        print(res.json())\n",
    "        raise HTTPException(status_code=500)\n",
    "\n",
    "    res = res.json()\n",
    "\n",
    "    keywords_str: str = res[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "    keywords = keywords_str.split(\"\\n\")\n",
    "    return [keyword for keyword in keywords if keyword.strip() != \"\"]\n",
    "\n",
    "keywords = keyword_gen(title, abstract)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec8b4f26-f6a8-4205-96ef-da58bc04ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Papers\n",
    "\n",
    "from typing import List\n",
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "\n",
    "ARXIV_URI = \"https://export.arxiv.org/api/query\"\n",
    "\n",
    "\n",
    "ARXIV_URI = \"https://export.arxiv.org/api/query\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"YourAppName/1.0 (Contact: your-email@example.com)\"\n",
    "}\n",
    "\n",
    "'''\n",
    "Sample query\tError Explanation\n",
    "http://export.arxiv.org/api/query?start=not_an_int\tstart must be an integer\n",
    "http://export.arxiv.org/api/query?start=-1\tstart must be >= 0\n",
    "http://export.arxiv.org/api/query?max_results=not_an_int\tmax_results must be an integer\n",
    "http://export.arxiv.org/api/query?max_results=-1\tmax_results must be >= 0\n",
    "http://export.arxiv.org/api/query?id_list=1234.1234\tmalformed id - see arxiv identifier explanation\n",
    "http://export.arxiv.org/api/query?id_list=condâ€”mat/0709123\tmalformed id - see arxiv identifier explanation\n",
    "\n",
    "'''\n",
    "def validate_arxiv_query_params(start: int, max_results: int):\n",
    "    if not isinstance(start, int) or start < 0:\n",
    "        raise ValueError(\"start must be a non-negative integer\")\n",
    "    if not isinstance(max_results, int) or max_results <= 0:\n",
    "        raise ValueError(\"max_results must be a positive integer\")\n",
    "\n",
    "def fetch_arxiv_data(query: str, start: int = 0, max_results: int = 10, retries: int = 3, delay: float = 3.0) -> List[dict]:\n",
    "    validate_arxiv_query_params(start, max_results)\n",
    "\n",
    "    params = {\n",
    "        'search_query': query,\n",
    "        'start': start,\n",
    "        'max_results': max_results\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(ARXIV_URI, params=params, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if not response.text.strip():\n",
    "                raise Exception(\"Empty response from arXiv\")\n",
    "\n",
    "            parsed_response = xmltodict.parse(response.text)\n",
    "            entries = parsed_response.get('feed', {}).get('entry', [])\n",
    "\n",
    "            if isinstance(entries, dict):\n",
    "                entries = [entries]\n",
    "\n",
    "            if not entries:\n",
    "                return []\n",
    "\n",
    "            papers = []\n",
    "            for entry in entries:\n",
    "                authors_data = entry.get('author', [])\n",
    "                if isinstance(authors_data, dict):\n",
    "                    authors = [authors_data.get('name', '')]\n",
    "                elif isinstance(authors_data, list):\n",
    "                    authors = [author.get('name', '') for author in authors_data]\n",
    "                else:\n",
    "                    authors = []\n",
    "\n",
    "                categories_data = entry.get('category', [])\n",
    "                if isinstance(categories_data, dict):\n",
    "                    categories = [categories_data.get('@term', '')]\n",
    "                elif isinstance(categories_data, list):\n",
    "                    categories = [cat.get('@term', '') for cat in categories_data]\n",
    "                else:\n",
    "                    categories = []\n",
    "\n",
    "                links_data = entry.get('link', [])\n",
    "                if isinstance(links_data, dict):\n",
    "                    links_data = [links_data]\n",
    "                links = {link.get('@rel', ''): link.get('@href', '') for link in links_data}\n",
    "\n",
    "                paper = {\n",
    "                    'id': entry.get('id', ''),\n",
    "                    'title': entry.get('title', '').strip(),\n",
    "                    'summary': entry.get('summary', '').strip(),\n",
    "                    'published': entry.get('published', ''),\n",
    "                    'updated': entry.get('updated', ''),\n",
    "                    'authors': authors,\n",
    "                    'categories': categories,\n",
    "                    'links': links\n",
    "                }\n",
    "\n",
    "                papers.append(paper)\n",
    "\n",
    "            time.sleep(delay)  # rate limit\n",
    "            return papers\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "                continue\n",
    "            raise Exception(f\"Request to arXiv failed after {retries} attempts: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while processing the response: {str(e)}\")\n",
    "\n",
    "def retrieve_papers(keyword, max_results=20):\n",
    "    return fetch_arxiv_data(query=keyword, max_results=max_results, start=0)\n",
    "\n",
    "papers_unfiltered = []\n",
    "for keyword in keywords:\n",
    "    papers_unfiltered = papers_unfiltered + retrieve_papers(keyword=keyword)\n",
    "\n",
    "# Remove repeated\n",
    "papers = []\n",
    "seen_ids = set()\n",
    "for paper in papers_unfiltered:\n",
    "    if paper['id'] not in seen_ids:\n",
    "        papers.append(paper)\n",
    "        seen_ids.add(paper['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6b54a0a-f1e0-464f-bfc1-2645edf08f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "papers count 12\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Relevant Papers\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def retrieve_relevant_papers(title: str, abstract: str, related_papers: List[Dict]) -> List[Dict[str,str]]:\n",
    "    '''\n",
    "    Retrieve the top k related papers based on the title and abstract of a given paper.\n",
    "    '''\n",
    "    if not related_papers:\n",
    "        return []\n",
    "    \n",
    "    # Generate embeddings for the title and abstract\n",
    "    query = f\"{title.strip()} {abstract.strip()}\"\n",
    "\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Prepare the contents of the related papers for embedding\n",
    "\n",
    "    paper_contents = []\n",
    "\n",
    "    for paper in related_papers:\n",
    "        paper_title = paper.get('title', '').strip()\n",
    "        paper_abstract = paper.get('summary', '').strip()\n",
    "        paper_contents.append(f\"{paper_title} {paper_abstract}\")\n",
    "\n",
    "    paper_embeddings = model.encode(paper_contents, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "\n",
    "    cosine_scores = util.cos_sim(query_embedding, paper_embeddings)[0]\n",
    "    min_papers = 8\n",
    "\n",
    "    thresholds = [0.75, 0.7, 0.65, 0.6]\n",
    "    for threshold in thresholds:\n",
    "        relevant_indices = (cosine_scores >= threshold).nonzero(as_tuple=True)[0]\n",
    "        if len(relevant_indices) >= min_papers or threshold == thresholds[-1]:\n",
    "            relevant_papers = [related_papers[i] for i in relevant_indices.tolist()]\n",
    "            return relevant_papers\n",
    "\n",
    "    return []  # fallback (shouldn't be reached if thresholds cover enough ground)top_papers = retrieve_relevant_papers(title=title, abstract=abstract, related_papers=papers)\n",
    "\n",
    "top_papers = retrieve_relevant_papers(title, abstract, papers)\n",
    "print(\"papers count\", len(top_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dc046f6-6b76-4062-b262-168aef1fd0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'authors': ['Jiaqi Wang',\n",
      "             'Huan Zhao',\n",
      "             'Zhenyuan Yang',\n",
      "             'Peng Shu',\n",
      "             'Junhao Chen',\n",
      "             'Haobo Sun',\n",
      "             'Ruixi Liang',\n",
      "             'Shixin Li',\n",
      "             'Pengcheng Shi',\n",
      "             'Longjun Ma',\n",
      "             'Zongjia Liu',\n",
      "             'Zhengliang Liu',\n",
      "             'Tianyang Zhong',\n",
      "             'Yutong Zhang',\n",
      "             'Chong Ma',\n",
      "             'Xin Zhang',\n",
      "             'Tuo Zhang',\n",
      "             'Tianli Ding',\n",
      "             'Yudan Ren',\n",
      "             'Tianming Liu',\n",
      "             'Xi Jiang',\n",
      "             'Shu Zhang'],\n",
      " 'categories': ['cs.CL', 'cs.AI'],\n",
      " 'id': 'http://arxiv.org/abs/2411.10137v1',\n",
      " 'links': {'alternate': 'http://arxiv.org/abs/2411.10137v1',\n",
      "           'related': 'http://arxiv.org/pdf/2411.10137v1'},\n",
      " 'published': '2024-11-15T12:23:12Z',\n",
      " 'summary': 'In this paper, we review legal testing methods based on Large '\n",
      "            'Language Models\\n'\n",
      "            '(LLMs), using the OPENAI o1 model as a case study to evaluate the '\n",
      "            'performance\\n'\n",
      "            'of large models in applying legal provisions. We compare current\\n'\n",
      "            'state-of-the-art LLMs, including open-source, closed-source, and '\n",
      "            'legal-specific\\n'\n",
      "            'models trained specifically for the legal domain. Systematic '\n",
      "            'tests are\\n'\n",
      "            'conducted on English and Chinese legal cases, and the results are '\n",
      "            'analyzed in\\n'\n",
      "            'depth. Through systematic testing of legal cases from common law '\n",
      "            'systems and\\n'\n",
      "            'China, this paper explores the strengths and weaknesses of LLMs '\n",
      "            'in\\n'\n",
      "            'understanding and applying legal texts, reasoning through legal '\n",
      "            'issues, and\\n'\n",
      "            'predicting judgments. The experimental results highlight both the '\n",
      "            'potential and\\n'\n",
      "            'limitations of LLMs in legal applications, particularly in terms '\n",
      "            'of challenges\\n'\n",
      "            'related to the interpretation of legal language and the accuracy '\n",
      "            'of legal\\n'\n",
      "            'reasoning. Finally, the paper provides a comprehensive analysis '\n",
      "            'of the\\n'\n",
      "            'advantages and disadvantages of various types of models, offering '\n",
      "            'valuable\\n'\n",
      "            'insights and references for the future application of AI in the '\n",
      "            'legal field.',\n",
      " 'title': 'Legal Evalutions and Challenges of Large Language Models',\n",
      " 'updated': '2024-11-15T12:23:12Z'}\n"
     ]
    }
   ],
   "source": [
    "__import__('pprint').pprint(top_papers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f03bb14-5ce5-4af2-b224-e85466617587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 12\n"
     ]
    }
   ],
   "source": [
    "# Extract Text From PDFs\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "def extract_text_from_pdf(pdf_source, filetype=None) -> str:\n",
    "    if filetype:\n",
    "        doc = fitz.open(stream=pdf_source, filetype=filetype)\n",
    "    else:\n",
    "        doc = fitz.open(pdf_source)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_url(url: str) -> str:\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        return extract_text_from_pdf(BytesIO(response.content), filetype=\"pdf\")\n",
    "    return \"\"\n",
    "\n",
    "def split_into_sections(text: str) -> Dict[str, str]:\n",
    "    sections = {\n",
    "        'title': '',\n",
    "        'abstract': '',\n",
    "        'introduction': '',\n",
    "        'methods': '',\n",
    "        'results': '',\n",
    "        'conclusion': '',\n",
    "    }\n",
    "\n",
    "    # Very crude split based on keywords\n",
    "    section_patterns = {\n",
    "        'abstract': r'(?i)\\babstract\\b',\n",
    "        'introduction': r'(?i)\\bintroduction\\b',\n",
    "        'methods': r'(?i)\\b(methodology|methods|approach)\\b',\n",
    "        'results': r'(?i)\\b(results|findings|experiments)\\b',\n",
    "        'conclusion': r'(?i)\\b(conclusion|discussion|summary)\\b'\n",
    "    }\n",
    "\n",
    "    current_section = 'title'\n",
    "    lines = text.split('\\n')\n",
    "    buffer = []\n",
    "\n",
    "    for line in lines:\n",
    "        line_clean = line.strip()\n",
    "        if not line_clean:\n",
    "            continue\n",
    "\n",
    "        matched = False\n",
    "        for sec, pattern in section_patterns.items():\n",
    "            if re.fullmatch(pattern, line_clean.lower()):\n",
    "                sections[current_section] = '\\n'.join(buffer).strip()\n",
    "                buffer = []\n",
    "                current_section = sec\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            buffer.append(line_clean)\n",
    "\n",
    "    # Save final section\n",
    "    sections[current_section] = '\\n'.join(buffer).strip()\n",
    "\n",
    "    return sections\n",
    "\n",
    "papers_extracted = []\n",
    "for paper in top_papers:\n",
    "    paper_links = paper.get('links')\n",
    "    if not paper_links:\n",
    "        continue\n",
    "    paper_links_related = paper_links.get('related')\n",
    "    if not paper_links_related:\n",
    "        continue\n",
    "    try:\n",
    "        full_text = extract_text_from_url(paper_links_related)\n",
    "        sectioned_text = split_into_sections(full_text)\n",
    "\n",
    "        paper_extracted = {\n",
    "            **paper,  # keep original metadata: title, authors, etc.\n",
    "            \"sections\": sectioned_text  # add structured content\n",
    "        }\n",
    "\n",
    "        papers_extracted.append(paper_extracted)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {paper.get('title')}: {e}\")\n",
    "print(len(top_papers), len(papers_extracted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d1728cf-6dbc-49de-bf00-e76bafc77a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections'])\n"
     ]
    }
   ],
   "source": [
    "print(papers_extracted[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "23ec85d7-c4ca-4b7c-b846-3d92281680d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize pdfs using gemini\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "def get_gemini_dict_response(prompt_text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Gemini API and returns the parsed JSON response as a dictionary.\n",
    "    Assumes GEMINI_API_URL is set in environment variables and includes necessary auth.\n",
    "    This function specifically handles cases where Gemini wraps JSON output in markdown code blocks.\n",
    "    \"\"\"\n",
    "    GEMINI_API_URL = os.environ.get(\"GEMINI_API_URL\")\n",
    "    if not GEMINI_API_URL:\n",
    "        raise ValueError(\"GEMINI_API_URL env var not set.\")\n",
    "\n",
    "    payload = {\"contents\": [{\"parts\": [{\"text\": prompt_text}]}]}\n",
    "    \n",
    "    try:\n",
    "        res = requests.post(GEMINI_API_URL, json=payload)\n",
    "        res.raise_for_status() # Raises HTTPError for bad responses (4xx or 5xx)\n",
    "        \n",
    "        response_data = res.json()\n",
    "\n",
    "        # Extract the text content from the Gemini response structure\n",
    "        # This is where the actual JSON string generated by Gemini will be\n",
    "        if \"candidates\" in response_data and \\\n",
    "           response_data[\"candidates\"] and \\\n",
    "           \"content\" in response_data[\"candidates\"][0] and \\\n",
    "           \"parts\" in response_data[\"candidates\"][0][\"content\"] and \\\n",
    "           response_data[\"candidates\"][0][\"content\"][\"parts\"] and \\\n",
    "           \"text\" in response_data[\"candidates\"][0][\"content\"][\"parts\"][0]:\n",
    "            \n",
    "            gemini_generated_text = response_data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "            \n",
    "            # --- CRITICAL FIX HERE ---\n",
    "            # Remove markdown code block delimiters if present\n",
    "            if gemini_generated_text.startswith(\"```json\") and gemini_generated_text.endswith(\"```\"):\n",
    "                # Remove the '```json\\n' from the start and '\\n```' from the end\n",
    "                json_string = gemini_generated_text[len(\"```json\\n\"):-len(\"\\n```\")]\n",
    "            else:\n",
    "                json_string = gemini_generated_text\n",
    "            # --- END CRITICAL FIX ---\n",
    "\n",
    "            # Now, attempt to parse the cleaned JSON string\n",
    "            return json.loads(json_string)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Gemini response did not contain expected content structure.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request to Gemini API: {e}\")\n",
    "        if 'res' in locals():\n",
    "            print(f\"Response content: {res.text}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON from Gemini's text output: {e}\")\n",
    "        if 'json_string' in locals():\n",
    "            print(f\"Attempted to parse: {json_string}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        raise\n",
    "\n",
    "def summarize_pdf(pdf_extracted):\n",
    "    sections = pdf_extracted['sections']\n",
    "    full_text_for_summary = f\"\"\"\n",
    "Title: {sections.get('title', '')}\n",
    "Abstract: {sections.get('abstract', '')}\n",
    "Introduction: {sections.get('introduction', '')}\n",
    "Methods: {sections.get('methods', '')}\n",
    "Results: {sections.get('results', '')}\n",
    "Conclusion: {sections.get('conclusion', '')}\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Summarize the following research paper into a JSON object.\n",
    "The JSON object should have the following structure:\n",
    "{{\n",
    "    \"title\": \"...\",\n",
    "    \"summary\": {{\n",
    "        \"problem_statement\": [\"bullet point 1\", \"bullet point 2\", ...],\n",
    "        \"methodology\": [\"bullet point 1\", \"bullet point 2\", ...],\n",
    "        \"key_findings\": [\"bullet point 1\", \"bullet point 2\", ...],\n",
    "        \"conclusion_recommendations\": [\"bullet point 1\", \"bullet point 2\", ...]\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Ensure all summaries are concise bullet points.\n",
    "\n",
    "Paper Text:\n",
    "---\n",
    "{full_text_for_summary}\n",
    "---\"\"\"\n",
    "    try:\n",
    "        response_dict = get_gemini_dict_response(prompt)\n",
    "        return json.dumps(response_dict, indent=2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "for paper_extracted in papers_extracted:\n",
    "    paper_extracted['summarized'] = summarize_pdf(paper_extracted)\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "874992a4-577d-4592-b2b7-b0bb6424f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in papers_extracted:\n",
    "    paper['summarized'] = json.loads(paper['summarized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71a8b382-d6b3-4398-870f-b2a7367f5d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type']),\n",
      " dict_keys(['id', 'title', 'summary', 'published', 'updated', 'authors', 'categories', 'links', 'sections', 'summarized', 'citation_intent', 'contribution_type'])]\n"
     ]
    }
   ],
   "source": [
    "__import__('pprint').pprint([paper.keys() for paper in papers_extracted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "94231811-1e72-4c02-ab6b-cda9a30bd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citation Intent and Contribution Tagging\n",
    "\n",
    "for paper in papers_extracted:\n",
    "    try:\n",
    "        prompt = f\"\"\"Given the following research paper summary and title, return a JSON object that contains:\n",
    "- `intent`: Citation intent â€” one of [\"Background\", \"Comparison\", \"Extension\", \"Criticism\", \"Application\", \"Future Work\", \"Other\"]\n",
    "- `contribution`: Contribution type â€” one of [\"Dataset\", \"Algorithm\", \"Framework\", \"Review\", \"Benchmark\", \"Survey\", \"System\", \"Theoretical Analysis\", \"Other\"]\n",
    "\n",
    "The JSON object should look like:\n",
    "{{\n",
    "  \"intent\": \"...\",\n",
    "  \"contribution\": \"...\"\n",
    "}}\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "Summary:\n",
    "{json.dumps(paper['summarized'], indent=2)}\n",
    "\n",
    "Only return the JSON object. Do not include any explanation or markdown formatting.\n",
    "\"\"\"\n",
    "        result = get_gemini_dict_response(prompt)\n",
    "        paper['citation_intent'] = result.get('intent')\n",
    "        paper['contribution_type'] = result.get('contribution')\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {paper['title']}: {e}\")\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8358ece-ad5a-499c-931c-fd88fa7c0e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': {'conclusion_recommendations': ['Further research and development '\n",
      "                                            \"are needed to improve LLMs' \"\n",
      "                                            'understanding of complex legal '\n",
      "                                            'language and reasoning.',\n",
      "                                            'Training methodologies should '\n",
      "                                            'integrate domain-specific legal '\n",
      "                                            'knowledge and strengthen '\n",
      "                                            'reasoning capabilities.',\n",
      "                                            'Develop more interpretable models '\n",
      "                                            'and incorporate human expertise '\n",
      "                                            'in the decision-making process.',\n",
      "                                            'Implement robust ethical review '\n",
      "                                            'mechanisms to ensure model '\n",
      "                                            'outputs adhere to laws, '\n",
      "                                            'regulations, and ethical '\n",
      "                                            'standards.',\n",
      "                                            'Consider regulatory policy '\n",
      "                                            'differences across countries when '\n",
      "                                            'adopting LLMs globally.'],\n",
      "             'key_findings': ['GPT-4o, Qwen2-7B-Instruct and O1-preview showed '\n",
      "                              'high human evaluation scores, indicating good '\n",
      "                              'alignment with human legal reasoning, but not '\n",
      "                              'always the highest ROUGE scores.',\n",
      "                              'Phi-3.5-mini-instruct and lawyer-llama-13b-v2 '\n",
      "                              'had the highest ROUGE scores, indicating strong '\n",
      "                              'lexical overlap with reference texts.',\n",
      "                              'The O1-preview model achieved the highest '\n",
      "                              'overall human evaluation score, demonstrating '\n",
      "                              'strong alignment with human judgment across '\n",
      "                              'diverse legal cases.',\n",
      "                              'Models generally achieved higher human '\n",
      "                              'evaluation scores in English legal texts.',\n",
      "                              'Legal-specific models demonstrate superior '\n",
      "                              'capabilities in understanding legal concepts, '\n",
      "                              'conducting legal reasoning, and generating '\n",
      "                              'legal text.'],\n",
      "             'methodology': ['Reviewed legal testing methods based on Large '\n",
      "                             \"Language Models (LLMs), using OPENAI's o1 model \"\n",
      "                             'as a case study.',\n",
      "                             'Compared open-source, closed-source, and '\n",
      "                             'legal-specific models.',\n",
      "                             'Conducted systematic tests on English and '\n",
      "                             'Chinese legal cases.',\n",
      "                             'Analyzed the strengths and weaknesses of LLMs in '\n",
      "                             'understanding legal texts, legal reasoning, and '\n",
      "                             'predicting judgments.',\n",
      "                             'Used 26 representative legal cases (13 Chinese, '\n",
      "                             '13 US) ensuring anonymization of personal data.'],\n",
      "             'problem_statement': ['Legal language is highly specialized, '\n",
      "                                   'making accuracy crucial for LLMs.',\n",
      "                                   'LLMs may absorb biases from training data, '\n",
      "                                   'leading to repercussions in legal '\n",
      "                                   'contexts.',\n",
      "                                   'Automation of legal decisions raises '\n",
      "                                   'ethical concerns and legal accountability '\n",
      "                                   'issues.',\n",
      "                                   'Effectively evaluating LLM performance '\n",
      "                                   'across legal systems and linguistic '\n",
      "                                   'environments is challenging.']},\n",
      " 'title': 'Legal Evaluations and Challenges of Large Language Models'}\n"
     ]
    }
   ],
   "source": [
    "__import__('pprint').pprint(papers_extracted[0]['summarized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8a1e2-f287-456b-ba15-529aff436732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bibtex_entry(paper):\n",
    "    authors = \" and \".join(paper[\"authors\"])\n",
    "    year = paper[\"published\"][:4]\n",
    "    arxiv_id = paper[\"id\"].split(\"/\")[-1]\n",
    "    \n",
    "    return f\"\"\"\n",
    "@article{{{arxiv_id},\n",
    "  title={{ {paper['title']} }},\n",
    "  author={{ {authors} }},\n",
    "  year={{ {year} }},\n",
    "  archivePrefix={{arXiv}},\n",
    "  eprint={{ {arxiv_id} }},\n",
    "  url={{ {paper['id']} }},\n",
    "  note={{ Preprint }},\n",
    "}}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a73257dd-e669-4c50-a52b-dc2714ea0336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The application of Natural Language Processing (NLP) to the legal domain has garnered increasing attention in recent years. Several studies have provided background and context for this burgeoning field. Zhong et al. (2020) offer a broad overview of how NLP benefits the legal system, summarizing the landscape of Legal Artificial Intelligence. Qin and Sun (2024) similarly explore the intersection of Large Language Models (LLMs) and legal systems in a concise survey. Focusing on the Indian legal landscape, Kalamkar et al. (2021) present a survey of Indian Legal NLP benchmarks, highlighting the specific challenges and opportunities within that context.\n",
      "\n",
      "A significant area of research involves the development of specialized datasets and benchmarks for legal NLP tasks. Nigam et al. (2024) introduce NyayaAnumana & INLegalLlama, a large dataset designed for Indian legal judgment prediction. Concurrently, Nigam et al. (2024) also presented PredEx, contributing to the rise of intelligent AI interpretation in Indian courts by releasing a novel dataset. Wang et al. (2024) contribute a benchmark specifically designed to evaluate the legal understanding and reasoning capabilities of LLMs. Zheng et al. (2025) further this line of research by introducing a reasoning-focused legal retrieval benchmark.\n",
      "\n",
      "Furthermore, researchers are actively developing and comparing algorithms tailored for legal information retrieval. Li et al. (2023) propose SAILER, a structure-aware pre-trained language model for legal case retrieval. Su et al. (2023) introduce Caseformer, a pre-training approach that leverages inter-case distinctions to improve retrieval performance. Upadhyay et al. (2025) present SynLexLM, an approach to scaling legal LLMs using synthetic data and curriculum learning.\n",
      "\n",
      "Finally, several studies focus on the application of these advancements to develop systems for legal assistance. Gupta et al. (2025) describe Legal Assist AI, a system leveraging transformer-based models for effective legal assistance. Panchal et al. (2025) introduce LawPal, a Retrieval Augmented Generation (RAG) based system aimed at enhancing legal accessibility in India.\n"
     ]
    }
   ],
   "source": [
    "# Build literature review\n",
    "\n",
    "def build_lit_review_prompt(papers: List[Dict[str, Any]]) -> str:\n",
    "    entries_text = \"\"\n",
    "    for paper in papers:\n",
    "        title = paper.get(\"title\", \"Unknown Title\")\n",
    "        authors = \", \".join(paper.get(\"authors\", [])[:3])\n",
    "        year = paper.get(\"published\", \"\")[:4] or \"Unknown Year\"\n",
    "        intent = paper.get(\"citation_intent\", \"Other\")\n",
    "        contribution = paper.get(\"contribution_type\", \"Other\")\n",
    "        summary = paper.get(\"summarized\", {})\n",
    "\n",
    "        entries_text += f\"\"\"\n",
    "ðŸ”¸ Title: {title}\n",
    "ðŸ‘¥ Authors: {authors}\n",
    "ðŸ“… Year: {year}\n",
    "ðŸŽ¯ Intent: {intent}\n",
    "ðŸ’¡ Contribution: {contribution}\n",
    "ðŸ“„ Summary:\n",
    "- Problem: {\"; \".join(summary.get(\"problem_statement\", []))}\n",
    "- Methodology: {\"; \".join(summary.get(\"methodology\", []))}\n",
    "- Findings: {\"; \".join(summary.get(\"key_findings\", []))}\n",
    "- Conclusion: {\"; \".join(summary.get(\"conclusion_recommendations\", []))}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    return f\"\"\"You are a research assistant helping write the literature review section of a research paper.\n",
    "\n",
    "Below are structured notes for several research papers. Your task is to write a coherent, academic-style literature review summarizing these works. Integrate them logically by theme, contribution type, or research direction (e.g., Background, Comparison, Extension...).\n",
    "\n",
    "Use proper citations (e.g., \"Wang et al. (2024)\") and mention paper titles only when helpful. Write in an academic tone with smooth transitions.\n",
    "\n",
    "Input papers:\n",
    "{entries_text}\n",
    "\n",
    "Begin the literature review now:\"\"\"\n",
    "\n",
    "\n",
    "def get_gemini_text_response(prompt_text: str) -> str:\n",
    "    GEMINI_API_URL = os.environ.get(\"GEMINI_API_URL\")\n",
    "    payload = {\"contents\": [{\"parts\": [{\"text\": prompt_text}]}]}\n",
    "    res = requests.post(GEMINI_API_URL, json=payload)\n",
    "    res.raise_for_status()\n",
    "    return res.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].strip()\n",
    "\n",
    "lit_review_prompt = build_lit_review_prompt(papers_extracted)\n",
    "lit_review_paragraph = get_gemini_text_response(lit_review_prompt)\n",
    "print(lit_review_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5923fcd-3678-486a-9dc0-44492fd5f85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
