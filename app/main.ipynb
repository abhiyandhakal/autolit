{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d487a634-f541-465c-8e1b-c3bd7aa99fce",
   "metadata": {},
   "source": [
    "# Autonomous Literature Review (AutoLit) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6eea6fc-5500-4b12-b15b-5ed6f96b1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test data\n",
    "title = \"Feasibility of Artificial Intelligence Driven Analysis in the Context of Nepalese Legal System\"\n",
    "abstract = \"We proposed an innovative solution through an Artificial Intelligence driven legal analysis customized to the utility of the Nepalese legal context. Using advanced machine learning (ML) models and Retrieval-Augmented Generation (RAG) techniques, the research provides legal insights, streamlines judicial processes, and enhances accessibility to legal information. The legal documents were processed to convert into JSON format, and then to convert into vector data. GPT-4o was used for query expansion and response generation, whereas text embeddings were generated through text-embedding-ada-002. Key features include efficient document retrieval and query expansion for enhanced search precision. The model performs well across different query types, achieving an ð¹1 score of 0.797 for rule-recall, 0.857 for rhetorical understanding, and 0.875 for interpretation-based queries. This work marks a significant step towards integrating AI into the legal domain of Nepal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5586560a-dfbe-4709-bb1d-dde563b0ebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI legal analysis', 'Nepalese legal system', 'Machine learning', 'RAG', 'Document retrieval', 'Query expansion', 'GPT-4o', 'Text embeddings']\n"
     ]
    }
   ],
   "source": [
    "# Keyword Generation\n",
    "import os\n",
    "from fastapi import HTTPException\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def keyword_gen(title: str, abstract: str) -> list[str]:\n",
    "    GEMINI_API_URL = os.environ.get(\"GEMINI_API_URL\") or \"\"\n",
    "    res = requests.post(GEMINI_API_URL, json={\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\n",
    "                \"text\": f\"\"\"\n",
    "                    Extract 5â€“10 relevant and concise keywords from the following\n",
    "                    research paper title and abstract. Each keyword should be 1â€“3\n",
    "                    words long. Return only the keywords as a plain list, one per\n",
    "                    line. Don't say anything else.\n",
    "\n",
    "                    Title: {title}\n",
    "\n",
    "                    Abstract: {abstract}\n",
    "                \"\"\"\n",
    "                }]\n",
    "            }]\n",
    "    })\n",
    "\n",
    "    if not res.ok:\n",
    "        print(res.json())\n",
    "        raise HTTPException(status_code=500)\n",
    "\n",
    "    res = res.json()\n",
    "\n",
    "    keywords_str: str = res[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "    keywords = keywords_str.split(\"\\n\")\n",
    "    return [keyword for keyword in keywords if keyword.strip() != \"\"]\n",
    "\n",
    "keywords = keyword_gen(title, abstract)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec8b4f26-f6a8-4205-96ef-da58bc04ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Papers\n",
    "\n",
    "from typing import List\n",
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "\n",
    "ARXIV_URI = \"https://export.arxiv.org/api/query\"\n",
    "\n",
    "\n",
    "ARXIV_URI = \"https://export.arxiv.org/api/query\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"YourAppName/1.0 (Contact: your-email@example.com)\"\n",
    "}\n",
    "\n",
    "'''\n",
    "Sample query\tError Explanation\n",
    "http://export.arxiv.org/api/query?start=not_an_int\tstart must be an integer\n",
    "http://export.arxiv.org/api/query?start=-1\tstart must be >= 0\n",
    "http://export.arxiv.org/api/query?max_results=not_an_int\tmax_results must be an integer\n",
    "http://export.arxiv.org/api/query?max_results=-1\tmax_results must be >= 0\n",
    "http://export.arxiv.org/api/query?id_list=1234.1234\tmalformed id - see arxiv identifier explanation\n",
    "http://export.arxiv.org/api/query?id_list=condâ€”mat/0709123\tmalformed id - see arxiv identifier explanation\n",
    "\n",
    "'''\n",
    "def validate_arxiv_query_params(start: int, max_results: int):\n",
    "    if not isinstance(start, int) or start < 0:\n",
    "        raise ValueError(\"start must be a non-negative integer\")\n",
    "    if not isinstance(max_results, int) or max_results <= 0:\n",
    "        raise ValueError(\"max_results must be a positive integer\")\n",
    "\n",
    "def fetch_arxiv_data(query: str, start: int = 0, max_results: int = 10, retries: int = 3, delay: float = 3.0) -> List[dict]:\n",
    "    validate_arxiv_query_params(start, max_results)\n",
    "\n",
    "    params = {\n",
    "        'search_query': query,\n",
    "        'start': start,\n",
    "        'max_results': max_results\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(ARXIV_URI, params=params, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if not response.text.strip():\n",
    "                raise Exception(\"Empty response from arXiv\")\n",
    "\n",
    "            parsed_response = xmltodict.parse(response.text)\n",
    "            entries = parsed_response.get('feed', {}).get('entry', [])\n",
    "\n",
    "            if isinstance(entries, dict):\n",
    "                entries = [entries]\n",
    "\n",
    "            if not entries:\n",
    "                return []\n",
    "\n",
    "            papers = []\n",
    "            for entry in entries:\n",
    "                authors_data = entry.get('author', [])\n",
    "                if isinstance(authors_data, dict):\n",
    "                    authors = [authors_data.get('name', '')]\n",
    "                elif isinstance(authors_data, list):\n",
    "                    authors = [author.get('name', '') for author in authors_data]\n",
    "                else:\n",
    "                    authors = []\n",
    "\n",
    "                categories_data = entry.get('category', [])\n",
    "                if isinstance(categories_data, dict):\n",
    "                    categories = [categories_data.get('@term', '')]\n",
    "                elif isinstance(categories_data, list):\n",
    "                    categories = [cat.get('@term', '') for cat in categories_data]\n",
    "                else:\n",
    "                    categories = []\n",
    "\n",
    "                links_data = entry.get('link', [])\n",
    "                if isinstance(links_data, dict):\n",
    "                    links_data = [links_data]\n",
    "                links = {link.get('@rel', ''): link.get('@href', '') for link in links_data}\n",
    "\n",
    "                paper = {\n",
    "                    'id': entry.get('id', ''),\n",
    "                    'title': entry.get('title', '').strip(),\n",
    "                    'summary': entry.get('summary', '').strip(),\n",
    "                    'published': entry.get('published', ''),\n",
    "                    'updated': entry.get('updated', ''),\n",
    "                    'authors': authors,\n",
    "                    'categories': categories,\n",
    "                    'links': links\n",
    "                }\n",
    "\n",
    "                papers.append(paper)\n",
    "\n",
    "            time.sleep(delay)  # rate limit\n",
    "            return papers\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2 ** attempt)\n",
    "                continue\n",
    "            raise Exception(f\"Request to arXiv failed after {retries} attempts: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"An error occurred while processing the response: {str(e)}\")\n",
    "\n",
    "def retrieve_papers(keyword, max_results=20):\n",
    "    return fetch_arxiv_data(query=keyword, max_results=max_results, start=0)\n",
    "\n",
    "papers_unfiltered = []\n",
    "for keyword in keywords:\n",
    "    papers_unfiltered = papers_unfiltered + retrieve_papers(keyword=keyword)\n",
    "\n",
    "# Remove repeated\n",
    "papers = []\n",
    "seen_ids = set()\n",
    "for paper in papers_unfiltered:\n",
    "    if paper['id'] not in seen_ids:\n",
    "        papers.append(paper)\n",
    "        seen_ids.add(paper['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6b54a0a-f1e0-464f-bfc1-2645edf08f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "papers count 12\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Relevant Papers\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def retrieve_relevant_papers(title: str, abstract: str, related_papers: List[Dict]) -> List[Dict[str,str]]:\n",
    "    '''\n",
    "    Retrieve the top k related papers based on the title and abstract of a given paper.\n",
    "    '''\n",
    "    if not related_papers:\n",
    "        return []\n",
    "    \n",
    "    # Generate embeddings for the title and abstract\n",
    "    query = f\"{title.strip()} {abstract.strip()}\"\n",
    "\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Prepare the contents of the related papers for embedding\n",
    "\n",
    "    paper_contents = []\n",
    "\n",
    "    for paper in related_papers:\n",
    "        paper_title = paper.get('title', '').strip()\n",
    "        paper_abstract = paper.get('summary', '').strip()\n",
    "        paper_contents.append(f\"{paper_title} {paper_abstract}\")\n",
    "\n",
    "    paper_embeddings = model.encode(paper_contents, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "\n",
    "    cosine_scores = util.cos_sim(query_embedding, paper_embeddings)[0]\n",
    "    min_papers = 8\n",
    "\n",
    "    thresholds = [0.75, 0.7, 0.65, 0.6]\n",
    "    for threshold in thresholds:\n",
    "        relevant_indices = (cosine_scores >= threshold).nonzero(as_tuple=True)[0]\n",
    "        if len(relevant_indices) >= min_papers or threshold == thresholds[-1]:\n",
    "            relevant_papers = [related_papers[i] for i in relevant_indices.tolist()]\n",
    "            return relevant_papers\n",
    "\n",
    "    return []  # fallback (shouldn't be reached if thresholds cover enough ground)top_papers = retrieve_relevant_papers(title=title, abstract=abstract, related_papers=papers)\n",
    "\n",
    "top_papers = retrieve_relevant_papers(title, abstract, papers)\n",
    "print(\"papers count\", len(top_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03bb14-5ce5-4af2-b224-e85466617587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
